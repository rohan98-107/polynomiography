5/17/2020

- Keep epsilon in mind for onBoundary
  (i) right now it's fixed at 5 but we want some way to scale it based on size of image

- What if we stop dealing with points now? What if we start performing operations on shapes?
  - smooth contours between density classes?

- Geometric Approach => line between neighbors, distance away from midpoint of line
  - but ordering is a problem here

PROGRESS UPDATE 1: So refineSingletons() does what we want it to do, it brings down the total number of
density classes by a few - now we need to figure out how to handle non-singleton cases that have more than 2
neighbors

    #for singleton in buckets

        #find neighbors** which are connected points that are already in density classes

        #if singleton has 0 neighbors**
            #remove it
        #if singleton has 1 neighbor**
            #add singleton to that neighbor's bucket
        #else if singleton has 2 neighbors**
            #find midpoint between N1 and N2 - call it M
            #if singleton is in a radius around M,
                #merge N1's bucket with N2's bucket and add M to the combined bucket
            #else
                #add to one of the neighbor's buckets
        #else
            #leave blank for now


- Tomorrow try to continue with singletons but this time address >= 2 neighbors
- Save non-singletons for later


5/18/20

-------------------------------------------------------------------
MEETING WITH CHUN

- set distance
- choose closest, dont worry about merging >= 2 classes at once
- our next step is to find the roots
- then after that we map convex hulls (shapes) to equations
- theoretical investigation of convex mappings to polynomials

By next meeting:
- radial angle (endpoints)
- 1 step of refinement
- try step 2 of refinement

------------------------------------------------------------------

- average distance method for multiple neighbors refinement did not work
- minimum distance method for multiple neighbors refinement first attempt also did not work
    - weirdly, they actually produced the same answers
    - RESOLVED: indentation issue

5/19/20

- today I will work on radial angle max/minimization to get the endpoints of each density class
- try to make use of the active contour model? but I think that already does what we are trying to do

RADIAL ANGLE DOESN'T SEEM LIKE IT WILL WORK -> lots of angles overlap
  -> might need to add distance parameter as a secondary condition

2nd attempt: with both distance and magnitude
<works partially>

5/20/20

- today I will try to work on the endpoint approach some more and try to fix it
  PROGRESS UPDATE 2: changed the "origin" to the bottom middle of the image; changed
  method so that we can set the origin on the fly, this will be useful later

- after testing some origins work slightly better than others, we will have to see how we can incorporate that
into combining the buckets further


5/21/20

-------------------------------------------------------------------
MEETING WITH CHUN

- paper on algebraic sets
  -> look up other papers within the same field

New Methods for Endpoints:
  (i) maximum distance within a density class
  (ii) set distance approach (I was gonna try anyway)
------------------------------------------------------------------

For Monday - just keep refining density classes the regular way and try to get it even lower
- did not code today -> just tried my best to read the paper on algebraic sets

5/22/20

- continued to go through the paper on algebraic sets
  => basically they have developed an approximation method for the closure of convex, algebraic sets...
  which we would want to prove that Newton's basins actually are before trying to deconstruct a non-
  symmetric image into a polynomiograph

- now for the code, which I am going to try two ways,
(1) I will try to compare the endpoint methods of radial angle and maximal distance
(2) Then I will try to merge non-singleton density classes using set distances
(2.5) We can try to incorporate the endpoints into our bucket merging

- update on part (1): radial angle and maximal distance give VERY different answer, but idk which is right

- Next I was trying to work on the part where I merge density classes together based on inf(set distance)
but realized that a more elegant way would be to minimize the length of the shortest path between every point
in the density class -> however this is just the Traveling Salesman problem -> not exactly feasible

- In order to use the inf distance method we would need to hardcode an epsilon and we have no way to
generalize that epsilon. Additionally, stray points (outliers) would mess this up (but maybe not by a lot)

- I think the endpoints way is the best way to go for merging but first I'd have to figure out which endpoint
method works properly. I'll do that testing tonight and then wrap up for the week.

- Test Results: different endpoint methods
  (*) Radial Angle method => seems to work better and is giving me more defined endpoints
  (*) As suspected, greatest internal distance is more susceptible to outliers, which begs the question why are we
  getting outliers


5/26/20

- understood the gist of the paper on algebraic sets
- now I'm going to try and merge density classes of size > 1

- after some more testing it seems like the density classes themselves are incorrect in some places

- perhaps if I re-write densityClass itself we can get a lot better results, I'll try it and see what happens
  => this doesn't make any sense, what I have now with the buckets plus some outliers and the radial angle endpoint is
  a much better approximation

- this is where I'm stuck
  (1) set distance won't work because we don't have a criteria for when something is ready to be merged or not
  (2) we could try radial angle again between two endpoints
  (3) we could also set a stop condition to be when BOTH endpoints of each density class are on a border

- there's a lot to unpack here, especially with the new approaches found from the two papers I read:
  (*) theta bodies
  (*) hyperbolic geometry and properties of analyticity for newton's basins

5/27/20

-------------------------------------------------------------------
MEETING WITH CHUN

- fix density classes by adding the extra conditions
  => any points that don't meet the criteria (but are border points)
  treat as singletons
- compare it to the originals

Next Steps:
- Once the density classes are closed contours:
- There are two ways we can go from there:

(1) (i) Treat each DC as a convex hull (ii) find the minimum area and then
compute the centroid

(2) (i) Treat each DC as its own polynomial (ii) Consider the system of
polynomial equations (ii-a.) compute the overall solution (ii-b.) compute
each equation's solution (iii) compare the two (iv) analyze both convergence
and geometric properties

Sub-goal: develop some theory for either (1) or (2)
Goal: compare (1) and (2)

**If we can't get closed contours:
  - it's okay, we can just scrap (2) and try to get polynomial approximations
  via method (1) and we can focus more on the theory and geometric intutition

**Active Contour Modeling also seems more fruitful than I initially thought,
there is a lot of potential in that method and we can perhaps bypass the work
we've done so far and then study (1) and (2) in the context of convexified
regions that are generated by approximating the deformable contours
---------------------------------------------------------------------

- tried to impose the distance, angle conditions on the density class function -> let's pray it works
- also, make note of all the arbitrary epsilons we have chosen throughout the code
- lastly, rename variables to make more sense


** test results: I don't think it's working -> it seems pretty similar to what we got the first time
- looking at it closer, they are almost exactly the same

- what if I tried an O(n^2) method with a radius, similar to the proof of Morera's Thm.
- It's looking good!!!

- In morera refinement:
  -> we can actually just perform a merge right then and there but then risk overextending rather than
  refining what we already have
  -> I implemented the safer option first and just removed any potential bad points, leaving the good points
  to be merged back later with refineSingletons

  => Just kidding :( something is wrong here, I'm getting a lot of good points removed, will have to debug

5/29/20

- debugging morera refinement
  => it was a dumb issue, I just forgot to add the density classes which were already complete

- merging method works EXTREMELY well. As i suspected, a few points are missing entirely from the final result,
however, there is much more overall structure to work with and now that I have multiple density classes with overlapping
points, I can refine easier and I imagine the final answer will be something to behold (I just hope it doesn't merge
multiple regions together)

- I think this is good! On Monday Chun and I can compare the two different approaches

=> took a break for a meeting, but by EOD today, I want to figure out how to merge the sets with common elements

PROGRESS UPDATE #2: We essentially have an overestimation and an underestimation; both are doing what they are intended to do, however:
  (1) merging too aggressively is causing multiple density classes which are close together to get smushed, which is what I wanted
  to avoid, however there might be a workaround
  (2) the set difference removal seems like a safer approach, but will need multiple singleton refinements and may just bring us right
  back to where we started which we don't want

*Random Shower thought: we could change the radius as we traverse through the border instead of fixing it (im referring to morera
refinement) we could describe some function that depends on how close the previous point was, but I suspect that might cause us to
go back to the original issue


6/1/20

---------------------------------------------------------------------
MEETING WITH CHUN:

- merging is causing even more problems than we started with
- I think that the back-refinement is more fruitful but we need to see if it's actually capturing
and isolating all the problematic points
- heatmap based on class size
- the whole point is to separate our image into distinct regions

---------------------------------------------------------------------

- will try the reconstruction method but if that still doesn't work then
I'm going to focus my efforts on finding another way to segment the image

- maybe the problem is with the point dictionary! the connecting edges could be what's causing all this
- so scrapping that helped considerably
- we have a lot more density classes but at least they all make sense now

- now that the classes make sense, the radial endpoint method seems a lot more promising

- ok, so the stupid triangle edges were causing all the problems >:(
  => now that we have everything neighborhood-based, we can work on different methods of refinement
  => angle endpoint joining looks like it could work decently well

- for now I naively merged based on which density classes had commonalities, will have to make sure
that works

- I also picked two papers to read today:
  (1) active contour modeling - https://web.archive.org/web/20160112014330/http://ww.vavlab.ee.boun.edu.tr/courses/574/material/Variational%20Image%20Segmentation/kaas_snakes.pdf
  (2) image segmentation - https://pdfs.semanticscholar.org/587a/acc01a4c33f0fe7fb172f5db785f40522b57.pdf

- some other thoughts: sorting the points could be fruitful, although that might take
some unnecessary work

Another paper: https://link.springer.com/article/10.1007/s11042-017-5429-8

6/2/20

- I will try to continue merging our density classes together to give us something useful
- Seems like something quite interesting is happening where everytime I merge, density classes become more and more overlapping,
if that's the case, that's really good! We can keep merging until we get to a point where there is no overlap

- after some preliminary testing that does seem to be the case: we can write quick functions that allow sequentially merging these
density classes in clever ways; we can also have much greater control in terms of debugging and how aggressively we are reducing the number
of density classes

- overall I think this is WAY better than what we had before, and it was all due to the triangulation (i.e. edge dictionary) -_-

- the sorting doesn't seem to work well but if we're getting closed contours then that shouldn't matter much

- now I have to figure out where I'm losing information, some of the important contours are getting thrown out entirely and that's not good
  => maybe instead of removal of sets I can just have a cleanup function
    -> jk this won't work because I need to bring down the size of the buckets as we continue through the loops

- I could introduce sensitivities in the function and see what happens, let's try that:
  => so sensitivities make a difference but the over-aggressive merging is still causing a huge loss in data
  => we will definitely have to relax the stop condition of the sequential merging but I'm not entirely sure how yet

- So there are definitely ways that I can improve the merge to stop critical data from being lost
  => I think somewhere along the line it's throwing away completed density classes since they are disjoint from every other
  class intermediately - will discuss this with Chun tomorrow

  6/3/20

  ---------------------------------------------------------------------
  MEETING WITH CHUN:

  - (1): We need "complete" heuristic, something to tell the algo when a density class is complete
      => once we have that, we find the centroid and that is in the sequence which will approximate the root

  - ways to work with our DCs:
        (i) matrix of points
        (ii) algebraic set (polynomial ideal)

  Next Steps:
    1. complete heuristic
      i. figure out how to compute neighbors (try to not use edge dict)
      ii. "filling in" the density classes

    2. verification - using convexity here will probably need to happen
    (check scipy.spatial.convexhull documentation)
  ---------------------------------------------------------------------

  - Email to Professor:
      (i) image segmentation using entropy points and basins of attractions
        - basically sell it
        - coming up with the outline for our manuscript

- took most of the day off :)

6/4/20

- regarding 'filling in' the density classes with extra points, we would need
a sorted density class
  => because of the way we construct the density class, this shouldn't be extremely
  hard, but we would probably not be able to use sets, since these mess up order


- regarding endpoints, once we figure out how we're going to write the isComplete()
function, we can skip looking for endpoints on complete density classes because we shouldn't
need them anymore

- Cool! So i got naiveSort to work, now we have a sensible arrangement of our points
in each class

- Once I had the ordering, filling in the class was very easy as was making the
prediction about the EXTENDED density class
  => the final product is looking pretty good

- tomorrow I will keep working on the completion heuristic and also see if I can now
leverage my extended density classes in order to merge - perhaps I can perform multiple
calls of my fillBorder() function to keep extending arbitrarily larger and larger

6/5/20

- So something really strange happened when I cast all the classes to be sets,
the program (naiveMerge) basically stopped merging - I have to look into this

- Also, after filling in the border, the program becomes MUCH slower, so we may have to look
into making our functions faster now

- Still need a complete heuristic because critical information is getting taken out before we get
the chance to merge them together

- So, radialRefinement is still doing the right thing and it's improving the approximation once the
borders are filled in -> HOWEVER, because of the lack of a complete heuristic, we just end up extending
the same density classes again

- there seem to be lots of small issues here and there but tbh I'm too scared to fix them because
everything works the way it is right now

- "COMPLETE" HEURISTIC:
  => one idea for an isComplete == true is if the density class is one of the n-th biggest, but stuff like
     the small circles might get left out with a criteria like that

  => ok so I have a very simple fix in place, but it DRASTICALLY increases the number of density classes,
  so I may have to do a lot more merges before getting what I want

- getting rid of duplicates in a list helped quite a bit, however I'm still facing some issues, I think I'm going
to have to do a full on debug and see what the issue is

6/7/20

- FULL DEBUG RESULTS:

radialRefinement():
  (i) first pass works fine
  (ii) made change to 'grouping' where instead of having the safety condition, I just removed the original subset
  whenever I extended the superset
    -> this seems to be working although I think when I did it last time the program was hanging - let me try running
    it fully and see what happens

=> ok so radial refinement works well, I think it was just that weird grouping loop that was messing things up

naiveMerge():
  => seems to be working fine (functionally) but for some reason I'm getting duplicates, gotta figure out why this is

naiveSort():
  - fails when the class doesn't pass the vertical line test

=> there are a few kinks to be worked out and we need to make the algorithm faster by making my code better, but other
than that I think now it just becomes a matter of playing with our radii, # of fill-ins, sensitivties etc.

6/8/20

---------------------------------------------------------------------
MEETING WITH CHUN:

- barycenter of a finite set of points
  -> then compute the distance from the different centroids
  (this only works for convex objects)

- we are reaching the crux of the theory now, the next step is
to simply perform a sequential analysis of the centroids 
---------------------------------------------------------------------
